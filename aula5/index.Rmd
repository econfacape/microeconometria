---
title: "Aula 5 - Introdução à Econometria"
author: "João Ricardo F. de Lima"
date: "`r format(Sys.time(), '%d de %B de %Y.')`"
output: 
    html_document:
        theme: flatly
        number_sections: yes
        highlight: textmate
#        includes: 
#          in_header: "header.html"
        toc: yes
        toc_float:
          collapsed: yes
          smooth_scroll: yes 
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo       = TRUE,
  warning    = FALSE,
  message    = FALSE,
  fig.width  = 10, 
  fig.height = 7,
  fig.align  = "center",
  comment    = "#",
  size       = "normalsize"
  )
```

# Pressupostos do Modelo Clássico de Regressão Linear

<br>


O Modelo Clássico de Regressão Linear se baseia em algumas hipóteses:

1) O modelo de regressão é linear nos parâmetros;

2) Os regressores são fixos (não aleatórios) em amostras repetidas (análise de regressão é condicional aos valores de X, ou seja, $E(Y|X)$;

3) Dados os valores de X, $E(\mu|X)=0$: Isso significa que não importa qual valor escolhemos para x, o termo de erro $\mu$ não deve apresentar nenhum padrão sistemático e deve ter média igual a zero.

Considere o caso em que, incondicionalmente, $E(\mu)=0$, mas para valores altos e baixos de x, o termo de erro tende a ser positivo e para valores médios de x, o erro tende a ser negativo. Pode-se usar R para construir tal exemplo. 

Começamos criando um vetor contendo valores uniformemente distribuídos no intervalo $[−5,5]$. Isso pode ser feito com a função `runif()`. Também precisamos simular o termo de erro. Para isso, geramos números aleatórios normalmente distribuídos com média igual a 0 e variância igual a 1 usando `rnorm()`. Os valores de Y são obtidos como uma função quadrática dos  valores de X e do erro.

Depois de gerar os dados, estima-se um modelo de regressão simples e um modelo quadrático que também inclui o regressor $X^2$. Por fim, plota-se os dados simulados e adiciona-se a linha de regressão estimada de um modelo de regressão simples, bem como as previsões feitas com um modelo quadrático para comparar o ajuste graficamente.

<br>

```{r aula5_2a, warning=FALSE, message=FALSE}
# definir uma semente para tornar os resultados reproduzíveis
set.seed(321)

# simular os dados 
X <- runif(50, min = -5, max = 5)
u <- rnorm(50, sd = 1)  

# a verdadeira relação 
Y <- X^2 + 2 * X + u                

# estimar um modelo de regressão simples, mas incorreto 
mod_simple <- lm(Y ~ X)

# estimar um modelo de regressão quadrático, correto
mod_quadratic <- lm( Y ~ X + I(X^2) ) 

# Previsão usando modelo quadrático
prediction <- predict(lm(Y ~ X + I(X^2)), data.frame(X = sort(X)))

# plot dos resultados
par(mfrow=c(1,3))

plot( Y ~ X, col = "steelblue", pch = 20, xlab = "X", ylab = "Y")       
abline( mod_simple, col = "red") 

#red line = incorrect linear regression (this violates the first OLS assumption)
lines( sort(X), prediction)                                            
# black line = correct quadratic regression
plot( resid(mod_quadratic) ~ X , col = "steelblue", pch = 20, main=c("Quadratic regression","Residuals plotted against X"), ylim=c(-10,15) )

plot( resid(mod_simple) ~ X , col = "steelblue", pch = 20, 
main=c("Simple regression","Residuals plotted against X"), 
ylim=c(-10,15) )
```

<br>

O gráfico mostra o que significa $E(u_i|X_i)=0$ e por que não vale para o modelo linear: Usando o modelo quadrático (representado pela curva preta) se observa que não há desvios sistemáticos da observação da relação prevista. É plausível que a suposição não seja violada quando tal modelo é empregado. No entanto, usando um modelo de regressão linear simples, vemos que a suposição é provavelmente violada como $E(u_i|X_i)$.

4) Dados distribuídos de forma independente e idêntica (iid): A maioria dos esquemas de amostragem usados ao coletar dados de populações produz amostras iid.. Por exemplo, pode-se usar o gerador de números aleatórios de R para selecionar aleatoriamente os números de matrículas dos alunos da FACAPE e registrar a idade e as remunerações $(X_i,Y_i)$ dos alunos correspondentes. Este é um exemplo típico de amostragem aleatória simples e garante que todos os $(X_i,Y_i)$ são sorteados aleatoriamente da mesma população.

5) Grandes outliers (valores discrepantes) não são desejados: É fácil encontrar situações em que podem ocorrer observações extremas, ou seja, observações que se desviam consideravelmente da faixa usual dos dados. Tais observações são chamadas de *outliers*. Tecnicamente falando, esta suposição exige que $(X_i,Y_i)$ tenha uma curtose finita.

Casos comuns em que se quer excluir ou (se possível) corrigir tais valores discrepantes são quando eles são aparentemente erros de digitação, erros de conversão ou erros de medição. Mesmo que pareça que as observações extremas foram registradas corretamente, é aconselhável excluí-las antes de estimar um modelo, pois o MQO sofre de sensibilidade a outliers.

O que isto significa? Pode-se mostrar que observações extremas recebem grande peso na estimativa dos coeficientes de regressão desconhecidos ao usar MQO. Portanto, outliers podem levar a estimativas fortemente distorcidas de coeficientes de regressão.

O código a seguir usa dados de amostra gerados usando as funções de número aleatório do R rnorm() e runif(). Estima-se dois modelos de regressão simples, um baseado no conjunto de dados original e outro usando um conjunto modificado onde uma observação é alterada para um outlier e, em seguida, plota-se os resultados. Para entender o código completo, a função sort() classifica as entradas de um vetor numérico em ordem crescente.

O resultado é bastante impressionante: a linha de regressão estimada com o outlier (linha preta) difere muito daquela que ajusta bem os dados (linha vermelha)).

<br>

```{r aula5_2b, warning=FALSE, message=FALSE}
# definir uma semente para tornar os resultados reproduzíveis
set.seed(123)

# gerar os dados
X <- sort(runif(10, min = 30, max = 70))
Y <- rnorm(10 , mean = 200, sd = 50)
Y[9] <- 2000

# estima o modelo sem outlier
fit <- lm(Y ~ X)
summary(fit)

# estima o modelo com outlier
fit2 <- lm(Y[-9] ~ X[-9])

summary(fit2)

# plota os resultados
plot(Y ~ X)
abline(fit)
abline(fit2, col = "red")
```

<br>

Com isso, é possível demonstrar que o estimador de MQO não é viesado: $E(\hat{\beta}_0)=\beta_0$ e $E(\hat{\beta}_1)=\beta_1$. 

**Intepretação de não viesado**

Os coeficientes estimados podem ser menores ou maiores, dependendo da amostra que é o resultado de um sorteio aleatório.

No entanto, em média, eles serão iguais aos valores que caracterizam a verdadeira relação entre y e x na população.

"Em média" significa que, a amostragem foi repetida, ou seja, foram sorteadas amostras aleatórias e repetidas as estimações diversas vezes.

Em uma dada amostra, as estimativas podem diferir consideravelmente dos valores verdadeiros. A demonstração será feita na parte de regressão múltipla.


Continuando com as hipóteses do MCRL, além de saber que a distribuição amostral de $\hat{\beta}_1$ está centrada em torno de $\beta_1$, é importante saber o quão distante, em média, pode-se esperar que  $\hat{\beta}_1$ esteja de $\beta_1$:

4) A variância de cada observação $u$, dados os valores de X, é constante - *homocedástico*:
 
 \begin{equation} 
 var (u|x)=\sigma^2
\tag{2}
\end{equation}

O valor da variável independente não deve conter informação sobre a variabilidade dos fatores não observados.

Como $var(u|x)=E(u^2|x)-[E(u|x)]^2$ e $E(u|x)=0$, $\sigma^2=E(u^2|x)$. 

Portanto, $\sigma^2=E(u^2)=var(u)$, pois $E(u)=0$.

<br>

![Homocedasticidade](img_r/woold_homo.png){width=85%}

Fonte: Wooldridge, 2017.
<br>

Outras hipóteses do MCRL:

5. Não há autocorrelação entre os termos de erro.

\begin{equation}
cov (u_i,u_j|X)=0
\tag{3}
\end{equation}

6. Nao existe multicolinearidade perfeita; 

7. O modelo está corretamente especificado; 

8. Os resíduos seguem a distribuição \emph{Normal}:
  
\begin{equation}
u \sim N(0,\sigma^2)
\tag{4}
\end{equation}

<br>

# Normalidade dos Resíduos

<br>

O MCRL assume que cada $u_i$ é distribuído Normalmente com: 

**Média Zero**

$$
E(u_i)=0
$$
**Variância Constante** 

$$
E[u_i-E(u_i)]^2=E(u_i^2)= \sigma^2
$$

**Covariância Zero** 

$$
E{[(u_i-E(u_i)][u_j-E(u_j)]}=E(u_i,u_j) =0 \quad \forall \quad i\neq j
$$
<br>

ou dito de outra forma

<br>

\begin{equation}
u_i \sim N(0,\sigma^2)
\tag{5}
\end{equation}

<br>

se duas variáveis são normalmente distribuídas e não são autocorrelacionadas, são chamadas de independentes. Assim, pode-se reescrever (5) da seguinte forma: 
 
\begin{equation} 
 u_i \sim IID(0,\sigma^2)
\tag{6}
\end{equation}

<br>

## Teste de Normalidade de Jarque-Bera

<br>

Para se testar Normalidade dos resíduos pode-se fazer um histograma dos resíduos e observar seu comportamento ou fazer o **teste de Jarque-Bera (JB)** de Normalidade dos resíduos. O teste de Jarque-Bera tem como pressuposto que a amostra seja grande. 

<br>

O teste calcula a simetria da distribuição dos resíduos e a Curtose e usa a seguintes estatística de teste: 

<br>

\begin{equation}   
JB=n [\frac{S^2}{6}+ \frac{(K-3)^2}{24}]
\tag{7}
\end{equation}

que segue a distribuição de $\chi^2$ com 2 graus de liberdade. A **hipótese nula** do teste de JB é que *os resíduos seguem a distribuição Normal*.

<br>

### Exemplo no R

<br>

```{r aula5_3, warning=FALSE, message=FALSE}
#Limpa o Ambiente Global
rm(list=ls())

#Pode usar dados de outros softwares
library(wooldridge)
library(dplyr)
library(htmltools)
library(knitr)
library(kableExtra)
library(tseries)

#Carregar dados no computador
data('ceosal1')

# Estimação do Modelo
regressao1 <- lm(salary ~ roe, data=ceosal1)

#Teste de Normalidade de Jarque-Bera - precisa do pacote tseries
jarque.bera.test(regressao1$residuals)
```

Como o valor de probabilidade é menor do que 0,10, rejeitamos a hipótese nula, ou seja, os resíduos não seguem a distribuição Normal.

<br>

## Teste de Normalidade de Shapiro-Wilk

<br>

O teste de Shapiro-Wilk é um teste de normalidade que testa, com base na estatística W, se um conjunto de dados segue a distribuição Normal. A estatística é:

\begin{equation}   
W=\frac{(\sum_{i=1}^{n}a_ix_i)^2}{(\sum_{i=1}^{n}x_i-\bar{x})^2}
\tag{8}
\end{equation}

Em que $x_i$ o menor número da amostra; $\bar{x}$ é a média amostral; as constantes $a_i$ são constantes geradas pelas médias, variâncias e covariâncias das estatísticas de ordem de uma amostra de tamanho n de uma distribuição Normal.

### Exemplo no R

<br>

```{r aula5_4, warning=FALSE, message=FALSE}

# Estimação do Modelo
regressao1 <- lm(salary ~ roe, data=ceosal1)

#Teste de Normalidade de Shapiro-Wilk
shapiro.test(regressao1$residuals)
```

Como o valor de probabilidade é menor do que 0,10, rejeitamos a hipótese nula, ou seja, os resíduos não seguem a distribuição Normal.

<br>

## Outras análises que podem ser feitas com os resíduos

<br>

```{r aula5_5, warning=FALSE, message=FALSE}
#Salvando os residuos do modelo e os valores estimados
residuos1 <- regressao1$residuals
ajustados1 <- regressao1$fitted.values

# Fazendo um gráfico para verificar algum padrão
plot(ajustados1, residuos1)

# Outra possibilidade de gráfico para verificar algum padrão
residuos_quad1 = (residuos1)^2
plot(residuos1, residuos_quad1)

#Outras análises de resíduos

#Gráfico QQ
qqnorm(residuos1)
qqline(residuos1)

hist(residuos1) #histograma dos resíduos
```
